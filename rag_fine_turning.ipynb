{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e819d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 22.0.0\n",
      "Uninstalling pyarrow-22.0.0:\n",
      "  Successfully uninstalled pyarrow-22.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyarrow==19.0.0\n",
      "  Using cached pyarrow-19.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Using cached pyarrow-19.0.0-cp313-cp313-win_amd64.whl (25.2 MB)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.4.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.0\n",
      "    Uninstalling pyarrow-19.0.0:\n",
      "      Successfully uninstalled pyarrow-19.0.0\n",
      "Successfully installed pyarrow-22.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y pyarrow\n",
    "%pip install pyarrow==19.0.0\n",
    "%pip install datasets\n",
    "%pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c5048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e48970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infrastructure\n",
    "torch.cuda.is_available()   # False: pas de GPU sur ma machine\n",
    "device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a28d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des donnees\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_split = 'train[:15%]'\n",
    "test_split = 'test[:5%]'\n",
    "\n",
    "dataset = load_dataset('imdb', split={ 'train': train_split, 'test': test_split })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54b6bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][10]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ae39b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'movie', 'was', 'fantastic', '!', 'i', 'really', 'loved', 'it', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modèle et tokenizer à utiliser\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Exple\n",
    "texte = 'This movie was fantastic! I really loved it.'\n",
    "tokens = tokenizer.tokenize(texte)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed9108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer mes donnees\n",
    "\n",
    "MAX_LENGTH =256\n",
    "def tokenize_function(texte):\n",
    "    return tokenizer(texte, padding='max_length', truncation=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1573c364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  1045,\n",
       "  12524,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2013,\n",
       "  2026,\n",
       "  2678,\n",
       "  3573,\n",
       "  2138,\n",
       "  1997,\n",
       "  2035,\n",
       "  1996,\n",
       "  6704,\n",
       "  2008,\n",
       "  5129,\n",
       "  2009,\n",
       "  2043,\n",
       "  2009,\n",
       "  2001,\n",
       "  2034,\n",
       "  2207,\n",
       "  1999,\n",
       "  3476,\n",
       "  1012,\n",
       "  1045,\n",
       "  2036,\n",
       "  2657,\n",
       "  2008,\n",
       "  2012,\n",
       "  2034,\n",
       "  2009,\n",
       "  2001,\n",
       "  8243,\n",
       "  2011,\n",
       "  1057,\n",
       "  1012,\n",
       "  1055,\n",
       "  1012,\n",
       "  8205,\n",
       "  2065,\n",
       "  2009,\n",
       "  2412,\n",
       "  2699,\n",
       "  2000,\n",
       "  4607,\n",
       "  2023,\n",
       "  2406,\n",
       "  1010,\n",
       "  3568,\n",
       "  2108,\n",
       "  1037,\n",
       "  5470,\n",
       "  1997,\n",
       "  3152,\n",
       "  2641,\n",
       "  1000,\n",
       "  6801,\n",
       "  1000,\n",
       "  1045,\n",
       "  2428,\n",
       "  2018,\n",
       "  2000,\n",
       "  2156,\n",
       "  2023,\n",
       "  2005,\n",
       "  2870,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1996,\n",
       "  5436,\n",
       "  2003,\n",
       "  8857,\n",
       "  2105,\n",
       "  1037,\n",
       "  2402,\n",
       "  4467,\n",
       "  3689,\n",
       "  3076,\n",
       "  2315,\n",
       "  14229,\n",
       "  2040,\n",
       "  4122,\n",
       "  2000,\n",
       "  4553,\n",
       "  2673,\n",
       "  2016,\n",
       "  2064,\n",
       "  2055,\n",
       "  2166,\n",
       "  1012,\n",
       "  1999,\n",
       "  3327,\n",
       "  2016,\n",
       "  4122,\n",
       "  2000,\n",
       "  3579,\n",
       "  2014,\n",
       "  3086,\n",
       "  2015,\n",
       "  2000,\n",
       "  2437,\n",
       "  2070,\n",
       "  4066,\n",
       "  1997,\n",
       "  4516,\n",
       "  2006,\n",
       "  2054,\n",
       "  1996,\n",
       "  2779,\n",
       "  25430,\n",
       "  14728,\n",
       "  2245,\n",
       "  2055,\n",
       "  3056,\n",
       "  2576,\n",
       "  3314,\n",
       "  2107,\n",
       "  2004,\n",
       "  1996,\n",
       "  5148,\n",
       "  2162,\n",
       "  1998,\n",
       "  2679,\n",
       "  3314,\n",
       "  1999,\n",
       "  1996,\n",
       "  2142,\n",
       "  2163,\n",
       "  1012,\n",
       "  1999,\n",
       "  2090,\n",
       "  4851,\n",
       "  8801,\n",
       "  1998,\n",
       "  6623,\n",
       "  7939,\n",
       "  4697,\n",
       "  3619,\n",
       "  1997,\n",
       "  8947,\n",
       "  2055,\n",
       "  2037,\n",
       "  10740,\n",
       "  2006,\n",
       "  4331,\n",
       "  1010,\n",
       "  2016,\n",
       "  2038,\n",
       "  3348,\n",
       "  2007,\n",
       "  2014,\n",
       "  3689,\n",
       "  3836,\n",
       "  1010,\n",
       "  19846,\n",
       "  1010,\n",
       "  1998,\n",
       "  2496,\n",
       "  2273,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2054,\n",
       "  8563,\n",
       "  2033,\n",
       "  2055,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2003,\n",
       "  2008,\n",
       "  2871,\n",
       "  2086,\n",
       "  3283,\n",
       "  1010,\n",
       "  2023,\n",
       "  2001,\n",
       "  2641,\n",
       "  26932,\n",
       "  1012,\n",
       "  2428,\n",
       "  1010,\n",
       "  1996,\n",
       "  3348,\n",
       "  1998,\n",
       "  16371,\n",
       "  25469,\n",
       "  5019,\n",
       "  2024,\n",
       "  2261,\n",
       "  1998,\n",
       "  2521,\n",
       "  2090,\n",
       "  1010,\n",
       "  2130,\n",
       "  2059,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2915,\n",
       "  2066,\n",
       "  2070,\n",
       "  10036,\n",
       "  2135,\n",
       "  2081,\n",
       "  22555,\n",
       "  2080,\n",
       "  1012,\n",
       "  2096,\n",
       "  2026,\n",
       "  2406,\n",
       "  3549,\n",
       "  2568,\n",
       "  2424,\n",
       "  2009,\n",
       "  16880,\n",
       "  1010,\n",
       "  1999,\n",
       "  4507,\n",
       "  3348,\n",
       "  1998,\n",
       "  16371,\n",
       "  25469,\n",
       "  2024,\n",
       "  1037,\n",
       "  2350,\n",
       "  18785,\n",
       "  1999,\n",
       "  4467,\n",
       "  5988,\n",
       "  1012,\n",
       "  2130,\n",
       "  13749,\n",
       "  7849,\n",
       "  24544,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized = dataset.map(lambda x: tokenize_function(x['text']))\n",
    "dataset_tokenized['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd30d8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Modèle de génération de texte\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10fa0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config    # Affiche la configuration du modèle\n",
    "model.num_parameters()  # Nombre de paramètres du modèle\n",
    "\n",
    "# Mettre le modèle sur le bon device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36420b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1516, 0.3402],\n",
       "        [0.1528, 0.3460]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test du modèle avec un texte d'exemple\n",
    "\n",
    "example_text = [\"This movie is beautiful.\", \"This movie is terrible.\"]\n",
    "# Tokenization\n",
    "tokens = tokenizer(example_text, padding='max_length', truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "tokens\n",
    "model(**tokens).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ec285c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning du modèle avec Trainer\n",
    "# Configuration de l'entraînement\n",
    "variable_entrainnement = TrainingArguments(\n",
    "    # Repértoires de sortie et de logs\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    # Paramètres d'entraînement\n",
    "    num_train_epochs=3,                 # nombre d'époques\n",
    "    per_device_train_batch_size=2,      # taille de batch pour l'entraînement\n",
    "    per_device_eval_batch_size=2,       # taille de batch pour l'évaluation\n",
    "    learning_rate=2e-5,                 # taux d'apprentissage (il doit être petit pour éviter de détruire les connaissances préalables du modèle)\n",
    "    weight_decay=0.01,                  # taux de décroissance du poids\n",
    "    # Optimisation du modèle\n",
    "    warmup_steps=100,                   # nombre de pas de warm-up(Optimise du modele 100fois)\n",
    "    lr_scheduler_type=\"linear\",         # type de scheduler (l'optimisation se fait de manière linéaire)\n",
    "    # Evaluation et sauvegarde du modèle\n",
    "    eval_strategy=\"epoch\",              # évaluer à la fin de chaque époque\n",
    "    save_strategy=\"epoch\",              # sauvegarder le modèle à la fin de chaque époque\n",
    "    load_best_model_at_end=True,        # charger le meilleur modèle à la fin de l'entraînement\n",
    "    metric_for_best_model=\"accuracy\",   # métrique pour déterminer le meilleur modèle\n",
    "    \n",
    "    save_total_limit=2,                 # nombre maximum de modèles sauvegardés\n",
    "    \n",
    "    # Enregistrement des logs\n",
    "    logging_strategy=\"steps\",           # stratégie de logging (enregistrer les logs à chaque étape)\n",
    "    logging_steps=10,                   # enregistrer les logs tous les 10 pas\n",
    "    logging_first_step=True,            # enregistrer le premier pas\n",
    "    report_to=\"tensorboard\"             # utiliser TensorBoard pour le reporting\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "518f6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c82660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15660\\3684593378.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1556' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1556/5625 2:09:28 < 5:39:01, 0.20 it/s, Epoch 0.83/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data collator pour le padding : cela permet de gérer les séquences de longueurs différentes dans un batch\n",
    "datacollator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,          # activer le padding\n",
    "    return_tensors='pt'    # retourner des tenseurs PyTorch\n",
    "    )\n",
    "\n",
    "# Entrainement du modèle\n",
    "trainer = Trainer(\n",
    "    model=model,                                # le modèle à entraîner\n",
    "    args=variable_entrainnement,                # les arguments d'entraînement\n",
    "    train_dataset=dataset_tokenized['train'],   # les donnees d'entraînement\n",
    "    eval_dataset=dataset_tokenized['test'] ,    # les donnees d'évaluation\n",
    "    tokenizer=tokenizer,                        # le tokenizer utilisé\n",
    "    compute_metrics=compute_metrics,            # fonction de calcul des métriques\n",
    "    data_collator=datacollator                  # le data collator pour le padding( pour gérer les séquences de longueurs différentes)\n",
    ")\n",
    "\n",
    "# Fine-tuning du modèle\n",
    "fineModel = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395aab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 5938.8443,\n",
       " 'train_samples_per_second': 1.894,\n",
       " 'train_steps_per_second': 0.947,\n",
       " 'total_flos': 1479999686400000.0,\n",
       " 'train_loss': 0.0035185262354989794,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fineModel.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf8b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/User/Desktop/Mes_Projets/Backend/Projects_Django/Test/.tf/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4687998373119626e-05, 'eval_accuracy': 1.0, 'eval_runtime': 57.2391, 'eval_samples_per_second': 21.838, 'eval_steps_per_second': 10.919, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation du modèle\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b25dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement du modèle\n",
    "path = './models'\n",
    "trainer.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b74e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0844, -5.4209]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test du modèle enregistré\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "# model\n",
    "text_1 = \"I really enjoyed this movie. It was fantastic!\"\n",
    "text_2 = \"I hated this movie. It was awful!\"\n",
    "inputs = tokenizer(text_1, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_text(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    text_input = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=MAX_LENGTH).to(device)\n",
    "    pred = model(**text_input)\n",
    "    prediction = torch.nn.functional.softmax(pred.logits, dim=-1)\n",
    "    predict_class = torch.argmax(prediction, dim=1).item()\n",
    "    # print(predict_class)\n",
    "    return predict_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b7b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_text(\"I hated this movie. It was awful!\", model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b44a3",
   "metadata": {},
   "source": [
    "# Exercice 1: Fine-tuning d'un modèle multilingue pour le wolof\n",
    "1. Telecharger un model multilangue sur huggingface\n",
    "2. Tester le modele avec un texte en wolof\n",
    "3. Creer un dataset en wolof sous format: description, reponse\n",
    "4. Fine-tuner le modele sur ce dataset\n",
    "5. tester le nouveau modele avec un texte en wolof\n",
    "\n",
    "# Exercice 2: Fine-tuning d'un modèle de question reponse pour le wolof\n",
    "1. Telecharger un model question reponse sur huggingface (Anglais, Francais, vice versa) ex: distilbert-base-uncased-distilled-squad\n",
    "2. Creer un dataset de question reponse en (Francais-Wolof, vice versa)\n",
    "3. Fine-tuner le modele sur ce dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/User/Desktop/Mes_Projets/Backend/Projects_Django/Test/.tf/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Exo1\n",
    "#1 Telechargement du model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name_for_classification = \"FacebookAI/xlm-roberta-base\"\n",
    "model_name_for_gereneration = \"google/mt5-base\"\n",
    "model_name_mistralai = \"mistralai/Mistral-7B-instruct-v0.2\"\n",
    "\n",
    "tokenizer_mistralai = AutoTokenizer.from_pretrained(model_name_mistralai)\n",
    "model_mistralai = \n",
    "\n",
    "# model_for_classification = AutoModelForSequenceClassification.from_pretrained(model_name_for_classification)\n",
    "# model_for_generation = AutoModelForSeq2SeqLM.from_pretrained(model_name_for_gereneration)\n",
    "\n",
    "# tokenizer_for_classification = AutoTokenizer.from_pretrained(model_name_for_classification)\n",
    "# tokenizer_for_generation = AutoTokenizer.from_pretrained(model_name_for_gereneration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be596d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEST 1: Modèle de CLASSIFICATION\n",
      "==================================================\n",
      "Texte: Taay dama contane\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "#2 Tester le model avec un texte en Wolof\n",
    "texte_1 = \"Taay dama contane\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TEST 1: Modèle de CLASSIFICATION\")\n",
    "print(\"=\"*50)\n",
    "# Test du modèle de classification\n",
    "tokens_class = tokenizer_for_classification(\n",
    "    texte_1, \n",
    "    return_tensors=\"pt\", \n",
    "    padding='max_length', \n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH\n",
    "    )\n",
    "print(f\"Texte: {texte_1}\")\n",
    "predit_class = model_for_classification(**tokens_class).logits\n",
    "print(predit_class.argmax(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f9988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples: 20\n",
      "Positifs: 10\n",
      "Négatifs: 10\n",
      "\n",
      "Premiers exemples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dama contane lool, sama liggéey dafa baax bi.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Film bi dafa interessant, man dama ko bëgg.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sama xarit dafa baax lool, dama ko sopp.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lekk bi dafa neex, jërëjëf.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jàng bi dafa am solo, prof bi dafa baax.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dama bëgg Dakar, dëkk bi dafa rafet.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yaram dafa fi rekk, alhumdulillah.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Làmb bi dafa am solo lool, ñu baax.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Suma waxtu ci université bi dafa nekk baax.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dama contane ci sama ndey, dafa ma jëkkër.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text  label\n",
       "0  Dama contane lool, sama liggéey dafa baax bi.      1\n",
       "1    Film bi dafa interessant, man dama ko bëgg.      1\n",
       "2       Sama xarit dafa baax lool, dama ko sopp.      1\n",
       "3                    Lekk bi dafa neex, jërëjëf.      1\n",
       "4       Jàng bi dafa am solo, prof bi dafa baax.      1\n",
       "5           Dama bëgg Dakar, dëkk bi dafa rafet.      1\n",
       "6             Yaram dafa fi rekk, alhumdulillah.      1\n",
       "7            Làmb bi dafa am solo lool, ñu baax.      1\n",
       "8    Suma waxtu ci université bi dafa nekk baax.      1\n",
       "9     Dama contane ci sama ndey, dafa ma jëkkër.      1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Creer un dataset en wolof sous format: texte, sentiment (0=négatif, 1=positif)\n",
    "data = {\n",
    "    \"text\": [\n",
    "        # Sentiments POSITIFS (label = 1)\n",
    "        \"Dama contane lool, sama liggéey dafa baax bi.\",\n",
    "        \"Film bi dafa interessant, man dama ko bëgg.\",\n",
    "        \"Sama xarit dafa baax lool, dama ko sopp.\",\n",
    "        \"Lekk bi dafa neex, jërëjëf.\",\n",
    "        \"Jàng bi dafa am solo, prof bi dafa baax.\",\n",
    "        \n",
    "        # Sentiments NÉGATIFS (label = 0)\n",
    "        \"Dama sonn lool, tee sama xol néxoul.\",\n",
    "        \"Dama togg, liggéey bi dafa méti.\",\n",
    "        \"Maa ngi dem ci université, te dama bëggul liggéey.\",\n",
    "        \"Film bi dafa ndaw, man dama soone ko.\",\n",
    "        \"Sama xarit dafa mënul, te dama sonnal.\"\n",
    "    ],\n",
    "    \"label\": [\n",
    "        # Labels pour sentiments positifs\n",
    "        1, 1, 1, 1, 1,\n",
    "        # Labels pour sentiments négatifs\n",
    "        0, 0, 0, 0, 0\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Créer un DataFrame pandas pour visualiser\n",
    "import pandas as pd\n",
    "dataset_wolof = pd.DataFrame(data)\n",
    "print(f\"Nombre d'exemples: {len(dataset_wolof)}\")\n",
    "print(f\"Positifs: {sum(dataset_wolof['label'] == 1)}\")\n",
    "print(f\"Négatifs: {sum(dataset_wolof['label'] == 0)}\")\n",
    "print(\"\\nPremiers exemples:\")\n",
    "dataset_wolof.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0235e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fine-tuner le modele sur ce dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"TEST 2: Modèle de GÉNÉRATION\")\n",
    "# print(\"=\"*50)\n",
    "# # Test du modèle de génération\n",
    "# tokens_gen = tokenizer_for_generation(texte_2, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "# print(f\"Texte d'entrée: {texte_2[:50]}...\")\n",
    "# model_for_generation.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Générer une réponse\n",
    "#     generated_ids = model_for_generation.generate(\n",
    "#         tokens_gen['input_ids'], \n",
    "#         max_length=50,\n",
    "#         num_beams=4,           # beam search pour meilleure qualité\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "    \n",
    "# # Décoder la sortie générée\n",
    "# generated_text = tokenizer_for_generation.decode(generated_ids[0], skip_special_tokens=True)\n",
    "# print(f\"Texte généré: {generated_text}\")\n",
    "# print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
